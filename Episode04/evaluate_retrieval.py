#!/usr/bin/env python3

"""
RAG Retrieval Evaluation Tool

This script evaluates the performance of RAG retrieval systems using questions generated
by text_question_generator.py. It supports FAISS, Pinecone, and Chroma vector stores,
with automatic embedding model detection and optional reranking.

Dependencies:
  - Python 3.x
  - External packages:
    - langchain-community: For FAISS support
    - langchain-openai: For OpenAI embeddings  
    - langchain-pinecone: For Pinecone support
    - langchain-chroma: For Chroma support (optional)
    - pandas: For data analysis and manipulation
    - matplotlib: For generating visualizations
    - seaborn: For enhanced visualizations
    - tqdm: For progress bars
    - requests: For Ollama API communication
    - sentence-transformers: For HuggingFace reranking models (optional)
    - torch: For HuggingFace model inference (optional)

Installation:
  pip install langchain-community langchain-openai pandas matplotlib seaborn tqdm requests

Usage:
  ./evaluate_embeddings.py --questions test_questions.json --store faiss [options]

Options:
  --questions QUESTIONS_FILE   Path to test questions JSON file (generated by text_question_generator.py)
  --store {faiss,pinecone,chroma}  Vector store type (default: faiss)
  --faiss-path PATH            FAISS index directory path
  --pinecone-key KEY           Pinecone API key
  --pinecone-index INDEX       Pinecone index name  
  --chroma-path PATH           Chroma database path
  --chroma-index INDEX         Chroma collection name
  --top-k TOP_K                Number of top results to consider (default: 4) 
  --output OUTPUT              Path to save report file
  --brief                      Generate brief report without details
  --debug                      Enable debug output
  --use-reranking              Enable reranking
  --reranker-type huggingface           Type of reranker (huggingface only)
  --rerank-model MODEL         Rerank model name
  --rerank-top-n INT           Number of documents to rerank (default: 100)
  --device DEVICE              Device for HuggingFace models (cpu, cuda, mps, etc.)

Input Format:
  JSON file from text_question_generator.py with structure:
  {
    "question": "The test question text",
    "passage": "Text passage that contains the answer", 
    "chunk_id": "unique_chunk_identifier",
    "sources": ["source_file_path"],
    "generated_with": "model-name",
    "chunk_index": 0,
    "total_chunks": 100,
    "file": "source_filename"
  }

Output:
  - Detailed MRR evaluation report (markdown format)
  - Visualization images of metrics
  - CSV files with detailed results for further analysis
  - JSON file with aggregated metrics
"""

import os
import sys
import json
import argparse
import requests
import datetime
import time
from typing import List, Dict, Any, Tuple, Union, Optional
from tqdm import tqdm
from collections import defaultdict
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Vector store imports
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_pinecone import PineconeVectorStore
from langchain_core.embeddings import Embeddings

# Optional imports
try:
    from langchain_chroma import Chroma
    import chromadb
    CHROMA_AVAILABLE = True
except ImportError:
    CHROMA_AVAILABLE = False

# Cohere support removed

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# Try to import HuggingFace dependencies
try:
    from sentence_transformers import CrossEncoder

    HUGGINGFACE_AVAILABLE = True
except ImportError:
    HUGGINGFACE_AVAILABLE = False
    print("Warning: sentence-transformers not available. Install with: pip install sentence-transformers torch")


class HuggingFaceReranker:
    """Client for reranking search results using HuggingFace cross-encoder models."""

    def __init__(self, model_name: str = "BAAI/bge-reranker-large", device: str = None):
        """Initialize the HuggingFace reranker.

        Args:
            model_name: Name or path of the HuggingFace model
            device: Device to use ('cpu', 'cuda', 'mps', etc.). If None, will auto-detect.
        """
        if not HUGGINGFACE_AVAILABLE:
            raise ImportError(
                "sentence-transformers is required for HuggingFace reranking. Install with: pip install sentence-transformers torch")

        self.model_name = model_name
        print(f"Loading HuggingFace reranker: {model_name}")

        # Auto-detect device if not specified
        if device is None:
            import torch
            if torch.cuda.is_available():
                device = "cuda"
                print(f"Using CUDA GPU for reranking")
            elif hasattr(torch, 'backends') and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = "mps"  # Apple Silicon (M1/M2/M3) GPU
                print(f"Using Apple Silicon MPS GPU for reranking")
            else:
                device = "cpu"
                print(f"Using CPU for reranking (this may be slow)")

        try:
            # Load the cross-encoder model
            print(f"Loading model on device: {device}")
            self.model = CrossEncoder(model_name, device=device)
            print(f"Successfully loaded {model_name}")
        except Exception as e:
            print(f"Error loading model {model_name}: {e}")
            print("Falling back to a smaller model...")
            try:
                # Fallback to a smaller, more reliable model
                fallback_model = "cross-encoder/ms-marco-MiniLM-L-6-v2"
                print(f"Trying fallback model {fallback_model} on device: {device}")
                self.model = CrossEncoder(fallback_model, device=device)
                self.model_name = fallback_model
                print(f"Successfully loaded fallback model: {fallback_model}")
            except Exception as e2:
                raise RuntimeError(f"Failed to load both primary and fallback models: {e2}")

    def rerank(self, query: str, documents: List[str], top_n: int = None) -> List[Dict]:
        """
        Rerank documents using HuggingFace cross-encoder model.

        Args:
            query: The search query
            documents: List of document texts to rerank
            top_n: Number of top results to return

        Returns:
            List of reranked results with relevance scores
        """
        if not documents:
            return []

        try:
            # Create query-document pairs for the cross-encoder
            pairs = [[query, doc] for doc in documents]

            # Get relevance scores
            scores = self.model.predict(pairs)

            # Create results with scores and original indices
            scored_results = []
            for i, (doc, score) in enumerate(zip(documents, scores)):
                scored_results.append({
                    'index': i,
                    'relevance_score': float(score),
                    'document': doc
                })

            # Sort by relevance score (descending)
            scored_results.sort(key=lambda x: x['relevance_score'], reverse=True)

            # Return top_n results
            if top_n:
                scored_results = scored_results[:top_n]

            return scored_results

        except Exception as e:
            print(f"Error during HuggingFace reranking: {e}")
            # Fallback to original order with dummy scores
            return [{'index': i, 'relevance_score': 1.0 - (i * 0.01), 'document': doc}
                    for i, doc in enumerate(documents)]


# CohereReranker class removed


def create_reranker(reranker_type="huggingface", model_name=None, device=None):
    """
    Factory function to create the appropriate reranker.

    Args:
        reranker_type: 'huggingface' (only supported type)
        model_name: Model name/path
        device: Device to use for HuggingFace models (auto-detected if None)

    Returns:
        HuggingFaceReranker instance
    """
    if reranker_type.lower() == "huggingface":
        default_model = "BAAI/bge-reranker-large" if model_name is None else model_name
        return HuggingFaceReranker(model_name=default_model, device=device)
    else:
        raise ValueError(f"Unknown reranker type: {reranker_type}. Only 'huggingface' is supported")


def debug_print(message, enable_debug=True):
    """Print debug messages if debugging is enabled."""
    if enable_debug:
        print(f"DEBUG: {message}")


class VectorStoreLoader:
    """Unified vector store loader supporting FAISS, Pinecone, and Chroma."""
    
    def __init__(self):
        self.vectorstore = None
        self.embeddings = None
        self.embedding_model = None
    
    def load_vector_store(self, store_type: str, **kwargs):
        """
        Load a vector store with automatic embedding model detection.
        
        Args:
            store_type: Type of vector store ('faiss', 'pinecone', 'chroma')
            **kwargs: Store-specific parameters
        """
        # Initialize embeddings (try to auto-detect for FAISS)
        self.embedding_model = 'text-embedding-3-small'  # default
        
        if store_type == 'faiss':
            faiss_path = kwargs.get('faiss_path') or os.getenv('FAISS_INDEX_PATH', 'faiss_index')
            if not os.path.exists(faiss_path):
                raise FileNotFoundError(f"FAISS index not found at '{faiss_path}'")
            
            # Try to auto-detect embedding model from metadata
            metadata_file = os.path.join(faiss_path, "index_metadata.json")
            if os.path.exists(metadata_file):
                try:
                    with open(metadata_file, 'r') as f:
                        metadata = json.load(f)
                        stored_embedding_model = metadata.get('embedding_model')
                        if stored_embedding_model:
                            self.embedding_model = stored_embedding_model
                            print(f"Auto-detected embedding model: {stored_embedding_model}")
                except Exception:
                    pass  # Fall back to default
        
        # Initialize embeddings based on detected/default model
        if self.embedding_model in ["bge-m3", "nomic-embed-text"]:
            # Use Ollama embeddings for non-OpenAI models
            try:
                # Test if Ollama is available
                response = requests.get("http://localhost:11434/api/tags", timeout=2)
                if response.status_code == 200:
                    self.embeddings = OllamaEmbeddings(model=self.embedding_model)
                    print(f"Using Ollama embeddings with model: {self.embedding_model}")
                else:
                    raise Exception("Ollama not available")
            except Exception as e:
                # Fallback to OpenAI if Ollama not available
                print(f"Warning: Ollama not available for {self.embedding_model} ({e}), falling back to OpenAI")
                self.embeddings = OpenAIEmbeddings(model='text-embedding-3-small')
                self.embedding_model = 'text-embedding-3-small'
        else:
            self.embeddings = OpenAIEmbeddings(model=self.embedding_model)
            print(f"Using OpenAI embeddings with model: {self.embedding_model}")
        
        # Load the vector store
        if store_type == 'faiss':
            print(f"Loading FAISS index from: {faiss_path}")
            self.vectorstore = FAISS.load_local(
                faiss_path,
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            print(f"Successfully loaded FAISS index with {self.vectorstore.index.ntotal} vectors")
            
        elif store_type == 'pinecone':
            pinecone_api_key = kwargs.get('pinecone_key') or os.getenv('PINECONE_API_KEY')
            pinecone_index = kwargs.get('pinecone_index') or os.getenv('PINECONE_INDEX')
            
            if not pinecone_api_key or not pinecone_index:
                raise ValueError("PINECONE_API_KEY and PINECONE_INDEX must be set for Pinecone store")
            
            self.vectorstore = PineconeVectorStore.from_existing_index(
                pinecone_index,
                self.embeddings
            )
            
        elif store_type == 'chroma':
            if not CHROMA_AVAILABLE:
                raise ImportError("Chroma dependencies not installed. Run: pip install langchain-chroma chromadb")
            
            chroma_path = kwargs.get('chroma_path') or os.getenv('CHROMA_PATH', './chroma_db')
            chroma_index = kwargs.get('chroma_index') or os.getenv('CHROMA_INDEX', 'default_index')
            
            if not os.path.exists(chroma_path):
                raise FileNotFoundError(f"Chroma database not found at '{chroma_path}'")
            
            client = chromadb.PersistentClient(path=chroma_path)
            self.vectorstore = Chroma(
                client=client,
                collection_name=chroma_index,
                embedding_function=self.embeddings,
                persist_directory=chroma_path
            )
        
        else:
            raise ValueError(f"Unsupported store type: {store_type}")
    
    def similarity_search_with_score(self, query: str, k: int = 4) -> List[Tuple[Any, float]]:
        """
        Perform similarity search and return documents with scores.
        
        Args:
            query: Query text
            k: Number of results to return
            
        Returns:
            List of (document, score) tuples
        """
        if not self.vectorstore:
            raise ValueError("Vector store not loaded")
            
        return self.vectorstore.similarity_search_with_score(query, k=k)


class OllamaEmbeddings(Embeddings):
    """Ollama embeddings using local models."""
    
    def __init__(self, model: str = "bge-m3", base_url: str = "http://localhost:11434"):
        self.model = model
        self.base_url = base_url.rstrip('/')
        
        # Test connection
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code != 200:
                raise RuntimeError("Ollama server not responding")
        except requests.exceptions.RequestException:
            raise RuntimeError("Ollama server not running. Start with: ollama serve")
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed search documents using Ollama."""
        embeddings = []
        for text in texts:
            response = requests.post(
                f"{self.base_url}/api/embeddings",
                json={"model": self.model, "prompt": text}
            )
            response.raise_for_status()
            embeddings.append(response.json()["embedding"])
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """Embed query text using Ollama."""
        response = requests.post(
            f"{self.base_url}/api/embeddings", 
            json={"model": self.model, "prompt": text}
        )
        response.raise_for_status()
        return response.json()["embedding"]


def load_questions(file_path: str) -> List[Dict[str, Any]]:
    """Load test questions from a JSON file."""
    with open(file_path, 'r', encoding='utf-8') as f:
        questions = json.load(f)
    print(f"Loaded {len(questions)} questions from {file_path}")
    return questions


def evaluate_retrieval_quality(vector_store_loader: VectorStoreLoader, questions: List[Dict[str, Any]],
                              top_k: int = 4,
                              output_file: str = None, detailed_report: bool = True,
                              debug: bool = False, use_reranking: bool = False,
                              reranker_type: str = "huggingface", rerank_model: str = None,
                              rerank_top_n: int = 100, device: str = None):
    """
    Evaluate retrieval quality using Mean Reciprocal Rank (MRR) and other metrics.
    
    Args:
        vector_store_loader: Loaded vector store instance
        questions: List of questions generated by text_question_generator.py
        top_k: Number of top results to consider for evaluation
        output_file: File to save the report to
        detailed_report: Whether to generate a detailed report
        debug: Whether to print debug information
        use_reranking: Whether to use reranking
        reranker_type: Type of reranker ('huggingface' only)
        rerank_model: Rerank model name/path
        rerank_top_n: Number of documents to rerank
        device: Device for HuggingFace models
        
    Returns:
        Tuple of (results, metrics)
    """
    # Initialize reranker if requested
    reranker = None
    if use_reranking:
        try:
            reranker = create_reranker(
                reranker_type=reranker_type,
                model_name=rerank_model,
                device=device
            )
            print(f"Initialized {reranker_type} reranker with model: {rerank_model or 'default'}")
        except Exception as e:
            print(f"Failed to initialize {reranker_type} reranker: {e}")
            print("Continuing without reranking...")
            use_reranking = False

    # Track results
    results = []
    successes = 0
    total = 0
    reciprocal_ranks = []
    hit_at_1 = 0
    hit_at_k = 0

    # Metrics for detailed report
    rank_counts = defaultdict(int)

    print(f"Evaluating retrieval quality for {len(questions)} questions...")
    print(f"Using question text as query")
    print(f"Top-k value: {top_k}")
    print(f"Embedding model: {vector_store_loader.embedding_model}")
    print(f"Reranking: {'Enabled' if use_reranking else 'Disabled'}")
    if use_reranking:
        print(f"Reranker type: {reranker_type}")
        print(f"Rerank model: {rerank_model or 'default'}")
        print(f"Rerank top-n: {rerank_top_n}")

    # Helper function for debug prints
    def debug_print_local(message):
        if debug:
            print(f"DEBUG: {message}")

    # Loop through questions
    for i, question_data in enumerate(tqdm(questions)):
        question_text = question_data['question']
        passage = question_data.get('passage', '')
        expected_chunk_id = question_data['chunk_id']
        sources = question_data.get('sources', [])
        
        # Use question text for query
        query_text = question_text
        
        # Track basic metadata
        
        debug_print_local(f"Question {i + 1}: {question_text}")
        debug_print_local(f"  Expected chunk_id: {expected_chunk_id}")
        debug_print_local(f"  Sources: {sources}")
        
        if not query_text.strip():
            print(f"Warning: Question {i + 1} has no query text")
            continue
            
        total += 1
        
        # Continue processing

        try:
            # Perform similarity search
            initial_results_count = rerank_top_n if use_reranking else top_k
            search_results = vector_store_loader.similarity_search_with_score(query_text, k=initial_results_count)
            
            debug_print_local(f"Found {len(search_results)} initial results")
            
            # Apply reranking if enabled
            if use_reranking and reranker and search_results:
                debug_print_local(f"Applying reranking to {len(search_results)} documents")
                
                # Extract documents for reranking
                documents = [doc.page_content for doc, score in search_results]
                
                # Rerank the documents
                reranked_results = reranker.rerank(
                    query=query_text,
                    documents=documents,
                    top_n=top_k  # Only return top_k after reranking
                )
                
                # Reorder results based on reranking
                reordered_results = []
                rerank_scores = []
                
                for rerank_result in reranked_results:
                    original_idx = rerank_result['index']
                    if original_idx < len(search_results):
                        doc, original_score = search_results[original_idx]
                        reordered_results.append((doc, original_score))
                        rerank_scores.append(rerank_result['relevance_score'])
                
                # Use reranked results
                final_results = reordered_results
                debug_print_local(f"Reranking complete. Using top {len(final_results)} results")
            else:
                # Use original results
                final_results = search_results[:top_k]
                rerank_scores = []
            
            # Find the rank of the expected chunk
            rank = None
            retrieved_chunks = []
            
            for idx, (doc, score) in enumerate(final_results):
                # Get chunk_id from document metadata (try both chunk_id and doc_id)
                doc_chunk_id = doc.metadata.get('chunk_id') or doc.metadata.get('doc_id')
                source_file = doc.metadata.get('source', doc.metadata.get('source_file', 'unknown'))
                
                # Calculate text overlap for additional analysis
                overlap = calculate_overlap(doc.page_content, passage) if passage else 0
                
                # Check if this matches the expected chunk
                is_match = (doc_chunk_id == expected_chunk_id)
                
                debug_print_local(f"  Result {idx + 1}: chunk_id={doc_chunk_id}, expected={expected_chunk_id}, match={is_match}")
                
                # Add rerank score if available
                rerank_score = None
                if use_reranking and idx < len(rerank_scores):
                    rerank_score = rerank_scores[idx]
                
                # Store chunk info
                chunk_info = {
                    "rank": idx + 1,
                    "chunk_id": doc_chunk_id,
                    "source_file": os.path.basename(source_file) if source_file else "unknown",
                    "overlap": overlap,
                    "overlap_str": f"{overlap:.1%}",
                    "is_match": is_match,
                    "similarity_score": float(score) if hasattr(score, '__float__') else score,
                    "text": doc.page_content[:150] + "..." if len(doc.page_content) > 150 else doc.page_content
                }
                
                if rerank_score is not None:
                    chunk_info["rerank_score"] = rerank_score
                
                retrieved_chunks.append(chunk_info)
                
                # Update rank if this is the first match
                if is_match and rank is None:
                    rank = idx + 1  # 1-based rank
                    break
            
            # Calculate metrics
            found = rank is not None
            if found:
                successes += 1
                reciprocal_ranks.append(1.0 / rank)
                if rank == 1:
                    hit_at_1 += 1
                if rank <= top_k:
                    hit_at_k += 1
                    
                # Update detailed counters
                rank_counts[rank] += 1
            else:
                reciprocal_ranks.append(0.0)
            
            # Create result record
            result = {
                "index": i + 1,
                "question": question_text,
                "query": query_text[:100] + ("..." if len(query_text) > 100 else ""),
                "query_type": "question",
                "expected_chunk_id": expected_chunk_id,
                "source_files": [os.path.basename(src) for src in sources],
                "found": found,
                "rank": rank,
                "success": found and rank <= top_k,
                "retrieved_chunks": retrieved_chunks,
                "reranked": use_reranking
            }
            
            results.append(result)
            
            # Print failure details for debugging
            if not found and debug:
                print(f"\nFailed to find chunk for question {i + 1}:")
                print(f"Query: {result['query']}")
                print(f"Expected chunk_id: {expected_chunk_id}")
                if retrieved_chunks:
                    best_chunk = retrieved_chunks[0]
                    print(f"Best match: {best_chunk['text']}")
                    print(f"Best match chunk_id: {best_chunk['chunk_id']}")
                    if use_reranking and 'rerank_score' in best_chunk:
                        print(f"Rerank score: {best_chunk['rerank_score']:.4f}")
                        
        except Exception as e:
            print(f"Error processing question {i + 1}: {e}")
            continue
    
    # Calculate final metrics
    if total == 0:
        return results, {"error": "No questions processed successfully"}
    
    # Avoid division by zero
    mrr = sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0.0
    hit_at_1_rate = hit_at_1 / total if total > 0 else 0.0
    hit_at_k_rate = hit_at_k / total if total > 0 else 0.0
    success_rate = successes / total if total > 0 else 0.0
    
    # Calculate MRR for different k values
    mrr_at_k = {}
    for k in range(1, top_k + 1):
        k_reciprocal_ranks = []
        for result in results:
            rank = result.get('rank')
            if rank and rank <= k:
                k_reciprocal_ranks.append(1.0 / rank)
            else:
                k_reciprocal_ranks.append(0.0)
        
        mrr_at_k[k] = sum(k_reciprocal_ranks) / len(k_reciprocal_ranks) if k_reciprocal_ranks else 0.0
    
    # Calculate additional metrics
    correct_ranks = [r['rank'] for r in results if r.get('rank')]
    avg_position = sum(correct_ranks) / len(correct_ranks) if correct_ranks else float('inf')
    
    # Top-k accuracy for different k values
    top_k_accuracy = {}
    for k in range(1, min(11, top_k + 1)):
        correct_in_top_k = sum(1 for r in results if r.get("rank") and r.get("rank") <= k)
        top_k_accuracy[f"top_{k}"] = correct_in_top_k / total if total > 0 else 0
    
    # Removed question type analysis
    
    # Rank distribution
    rank_distribution = dict(rank_counts)
    
    # Combine all metrics
    metrics = {
        "total_queries": total,
        "successful_queries": successes,
        "success_rate": success_rate,
        "mrr": mrr,
        "mrr_at_k": mrr_at_k,
        "hit_at_1": hit_at_1_rate,
        f"hit_at_{top_k}": hit_at_k_rate,
        "avg_position": avg_position,
        "top_k_accuracy": top_k_accuracy,
        "rank_distribution": rank_distribution,
        "reranking_enabled": use_reranking
    }
    
    if use_reranking:
        metrics["reranker_type"] = reranker_type
        metrics["rerank_model"] = rerank_model or "default"
        metrics["rerank_top_n"] = rerank_top_n
    
    # Generate report
    if detailed_report:
        print("Generating detailed report...")
    else:
        print("Generating brief report...")
    
    generate_report(results, metrics, output_file, vector_store_loader.embedding_model, top_k,
                    0.0, detailed_report, 0.0)  # No overlap thresholds for chunk_id matching
    
    # Generate visualizations
    generate_visualizations(results, metrics, output_file)
    
    return results, metrics


def print_failed_questions(results):
    """Print a detailed report of all questions that failed retrieval."""
    failed_results = [r for r in results if not r.get("success", False) and not r.get("impossible", False)]

    if not failed_results:
        print("\nNo failed questions found.")
        return

    print(f"\n===== FAILED QUESTIONS ({len(failed_results)}) =====")

    for i, result in enumerate(failed_results):
        print(f"\nFailed Question {i + 1}: Query {result['index']}")
        print(f"Question: {result['question']}")
        print(f"Query: {result['query']}")
        print(f"Expected source(s): {', '.join(result['source_files'])}")

        # Show best retrieved chunk
        if result["retrieved_chunks"]:
            best_chunk = result["retrieved_chunks"][0]
            print(f"Best match: {best_chunk['text']}")
            print(f"Source: {best_chunk['source_file']}")
            print(f"Overlap: {best_chunk['overlap_str']}")

            # Show rerank score if available
            if 'rerank_score' in best_chunk:
                print(f"Rerank score: {best_chunk['rerank_score']:.4f}")

            # Check if any chunks had matching sources but failed on overlap
            source_matches = [c for c in result["retrieved_chunks"] if
                              c.get("source_match", False) and not c.get("is_match", False)]
            if source_matches:
                print(f"\nFound {len(source_matches)} chunks with matching sources but insufficient overlap:")
                for idx, chunk in enumerate(source_matches[:3]):  # Show up to 3
                    print(f"- Rank {chunk['rank']}, Overlap: {chunk['overlap_str']}")
        print("-" * 80)  # Add a separator for better readability


def calculate_metrics(results, total, successes, mrr_sum, correct_ranks, rank_counts):
    """Calculate comprehensive metrics from the results."""
    # Only consider non-impossible questions for these metrics
    non_impossible_results = [r for r in results if not r.get("impossible", False)]
    non_impossible_total = len(non_impossible_results)

    # Basic metrics
    success_rate = successes / non_impossible_total if non_impossible_total > 0 else 0
    mrr = mrr_sum / non_impossible_total if non_impossible_total > 0 else 0
    avg_position = sum(correct_ranks) / len(correct_ranks) if correct_ranks else float('inf')

    # Top-k accuracy for different k values
    top_k_accuracy = {}
    for k in range(1, 11):  # Calculate top-1 through top-10
        correct_in_top_k = sum(1 for r in non_impossible_results if r.get("rank") and r.get("rank") <= k)
        top_k_accuracy[f"top_{k}"] = correct_in_top_k / non_impossible_total if non_impossible_total > 0 else 0

    # Rank distribution
    rank_distribution = {}
    for rank, count in rank_counts.items():
        rank_distribution[rank] = count

    # Combine all metrics
    metrics = {
        "total_queries": total,
        "successful_queries": successes,
        "success_rate": success_rate,
        "mrr": mrr,
        "avg_position": avg_position,
        "top_k_accuracy": top_k_accuracy,
        "rank_distribution": rank_distribution
    }

    return metrics


def generate_report(results, metrics, output_file, embedding_model, top_k,
                    overlap_threshold, detailed_report, impossible_overlap_threshold):
    """Generate a comprehensive report of the evaluation results."""
    report = []
    report.append("# Vector Search Quality Evaluation Report")
    report.append(f"\nEvaluation date: {datetime.datetime.now()}")
    report.append(f"Query type: Question text")
    report.append(f"Embedding model: {embedding_model}")
    report.append(f"Top-k value: {top_k}")

    # Add reranking info if enabled
    if metrics.get("reranking_enabled", False):
        report.append(f"Reranking: Enabled")
        report.append(f"Reranker type: {metrics.get('reranker_type', 'Unknown')}")
        report.append(f"Rerank model: {metrics.get('rerank_model', 'Unknown')}")
        report.append(f"Rerank top-n: {metrics.get('rerank_top_n', 'Unknown')}")
    else:
        report.append(f"Reranking: Disabled")

    report.append(
        f"Overlap threshold: {overlap_threshold if overlap_threshold > 0 else 'Disabled (using source match only)'}")
    report.append(f"Impossible question threshold: {impossible_overlap_threshold}")
    report.append(f"Total queries: {metrics['total_queries']}")

    # Basic stats
    report.append(f"Successful queries: {metrics['successful_queries']}")
    report.append(f"Success rate: {metrics['success_rate']:.1%}")

    # Overall metrics
    report.append("\n## Overall Metrics")
    report.append(f"Mean Reciprocal Rank (MRR): {metrics['mrr']:.4f}")
    report.append(f"Average position of correct chunk: {metrics['avg_position']:.2f}")

    # Top-k accuracy
    report.append("\n### Top-K Accuracy")
    for k in range(1, min(11, top_k + 1)):
        key = f"top_{k}"
        if key in metrics['top_k_accuracy']:
            report.append(f"- {key.replace('_', '-')}: {metrics['top_k_accuracy'][key]:.2%}")

    # Rank distribution
    if metrics['rank_distribution']:
        report.append("\n### Rank Distribution")
        total_for_ranks = sum(metrics['rank_distribution'].values())
        for rank, count in metrics['rank_distribution'].items():
            percentage = count / total_for_ranks if total_for_ranks > 0 else 0
            report.append(f"- Rank {rank}: {count} ({percentage:.1%})")



    # Impossible question details
    if 'impossible_questions' in metrics and metrics['impossible_questions'] > 0:
        report.append("\n### Impossible Question Verification")
        report.append(f"Total impossible questions: {metrics['impossible_questions']}")
        report.append(f"False positives: {metrics['false_positives']}")

        if metrics['impossible_questions'] > 0:
            fp_rate = metrics['false_positives'] / metrics['impossible_questions']
            report.append(f"False positive rate: {fp_rate:.1%}")

        # List impossible questions that were incorrectly matched
        if metrics['false_positives'] > 0:
            report.append("\nImpossible questions incorrectly matched:")
            fp_count = 0
            for result in results:
                if result.get("impossible", False) and any(
                        c.get("is_match", False) for c in result.get("retrieved_chunks", [])):
                    fp_count += 1
                    if fp_count <= 5:  # Limit to first 5
                        report.append(f"- {result['question']}")

            if fp_count > 5:
                report.append(f"(And {fp_count - 5} more...)")

    if detailed_report:
        # Detailed per-question results
        report.append("\n## Detailed Results")

        # Sort by success and rank (exclude impossible questions from this section)
        sorted_results = sorted(
            [r for r in results if not r.get("impossible", False)],
            key=lambda x: (0 if x.get("success", False) else 1, x.get("rank", float('inf')))
        )

        # Success cases first
        report.append("\n### Successful Retrievals")
        success_count = 0
        for i, result in enumerate(sorted_results):
            if result.get("success", False):
                success_count += 1
                if success_count <= 10:  # Limit to first 10 successes to keep the report manageable
                    report.append(f"\n#### Success {success_count}: Query {result['index']}")
                    report.append(f"Query: {result['query']}")
                    report.append(f"Expected source(s): {', '.join(result['source_files'])}")
                    report.append(f"Found at rank: {result['rank']}")

                    # Show the matching chunk
                    matching_chunk = next((c for c in result["retrieved_chunks"] if c.get("is_match", False)), None)
                    if matching_chunk:
                        report.append(f"Matching chunk: {matching_chunk['text']}")
                        report.append(f"Source: {matching_chunk['source_file']}")
                        report.append(f"Overlap: {matching_chunk['overlap_str']}")
                        if 'rerank_score' in matching_chunk:
                            report.append(f"Rerank score: {matching_chunk['rerank_score']:.4f}")

        if success_count > 10:
            report.append(f"\n(Showing 10 of {success_count} successful retrievals)")

        # Failure cases (excluding impossible questions)
        failed_results = [r for r in sorted_results if not r.get("success", False)]
        if failed_results:
            report.append("\n### Failed Retrievals")
            for i, result in enumerate(failed_results):
                if i < 10:  # Limit to first 10 failures
                    report.append(f"\n#### Failure {i + 1}: Query {result['index']}")
                    report.append(f"Query: {result['query']}")
                    report.append(f"Expected source(s): {', '.join(result['source_files'])}")

                    # Show best retrieved chunk
                    if result["retrieved_chunks"]:
                        best_chunk = result["retrieved_chunks"][0]
                        report.append(f"Best match: {best_chunk['text']}")
                        report.append(f"Source: {best_chunk['source_file']}")
                        report.append(f"Overlap: {best_chunk['overlap_str']}")
                        if 'rerank_score' in best_chunk:
                            report.append(f"Rerank score: {best_chunk['rerank_score']:.4f}")

                        # Show additional context for failed retrievals
                        if len(result["retrieved_chunks"]) > 1:
                            report.append(f"\nOther top results:")
                            for idx, chunk in enumerate(result["retrieved_chunks"][1:4]):  # Show up to 3 more
                                report.append(f"- Rank {chunk['rank']}, Source: {chunk['source_file']}, Overlap: {chunk['overlap_str']}")

            if len(failed_results) > 10:
                report.append(f"\n(Showing 10 of {len(failed_results)} failed retrievals)")

        # Show impossible question details
        if 'impossible_questions' in metrics and metrics['impossible_questions'] > 0:
            report.append("\n### Impossible Question Details")
            impossible_results = [r for r in results if r.get("impossible", False)]

            for i, result in enumerate(impossible_results):
                if i < 5:  # Limit to first 5
                    report.append(f"\n#### Impossible Question {i + 1}: {result['question']}")
                    report.append(f"Query: {result['query']}")
                    report.append(f"Expected source(s): {', '.join(result['source_files'])}")

                    # Show top retrieved chunks
                    if result["retrieved_chunks"]:
                        report.append("\nTop retrieved chunks:")
                        for j, chunk in enumerate(result["retrieved_chunks"][:3]):  # Show top 3
                            report.append(f"- Rank {chunk['rank']}, Source: {chunk['source_file']}")
                            report.append(f"  Text: {chunk['text']}")
                            report.append(f"  Overlap: {chunk['overlap_str']}")
                            report.append(f"  Chunk ID: {chunk.get('chunk_id', 'N/A')}")
                            report.append(f"  Is match: {chunk['is_match']}")
                            if 'rerank_score' in chunk:
                                report.append(f"  Rerank score: {chunk['rerank_score']:.4f}")

            if len(impossible_results) > 5:
                report.append(f"\n(Showing 5 of {len(impossible_results)} impossible questions)")

    else:
        # Add a brief section if it's a brief report
        report.append("\n### Brief Report")
        report.append(f"Total Queries Processed: {metrics['total_queries']}")

        report.append(f"Successful Queries: {metrics['successful_queries']}")
        report.append(f"Success Rate: {metrics['success_rate']:.1%}")

        report.append(f"MRR: {metrics['mrr']:.2f}")
        report.append(f"Top 1 Accuracy: {metrics['top_k_accuracy']['top_1']:.2%}")

        # Add source match statistics
        multi_source_count = sum(1 for r in results if len(r.get("source_files", [])) > 1)
        if multi_source_count > 0:
            report.append(
                f"\nQueries with multiple source files: {multi_source_count} ({multi_source_count / metrics['total_queries']:.1%})")

        # Add reranking impact if enabled
        if metrics.get("reranking_enabled", False):
            report.append(f"\nReranker type: {metrics.get('reranker_type', 'Unknown')}")
            report.append(f"Rerank model used: {metrics.get('rerank_model', 'Unknown')}")
            report.append(f"Documents reranked per query: {metrics.get('rerank_top_n', 'Unknown')}")

    # Save or print report
    report_text = "\n".join(report)
    if output_file:
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(report_text)
        print(f"Report saved to {output_file}")
    else:
        print(report_text)


def generate_visualizations(results, metrics, output_file=None):
    """Generate visualizations of the evaluation metrics using pandas and matplotlib."""

    # Create a figure with multiple subplots
    plt.style.use('ggplot')
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # Add reranking info to the title if enabled
    title_suffix = ""
    if metrics.get("reranking_enabled", False):
        reranker_type = metrics.get("reranker_type", "Unknown")
        model_name = metrics.get("rerank_model", "Unknown")
        if reranker_type == "huggingface":
            # For HF models, show just the model name without the full path for cleaner display
            display_name = model_name.split("/")[-1] if "/" in model_name else model_name
            title_suffix = f" (with {display_name} reranking)"
        else:
            title_suffix = f" (with {model_name} reranking)"
    fig.suptitle(f'Embedding Evaluation Metrics{title_suffix}', fontsize=16)

    # Plot 1: Top-K Accuracy
    df_topk = pd.DataFrame({
        'K': [int(k.split('_')[1]) for k in metrics['top_k_accuracy'].keys()],
        'Accuracy': list(metrics['top_k_accuracy'].values())
    })
    df_topk = df_topk.sort_values('K')

    sns.barplot(x='K', y='Accuracy', data=df_topk, ax=axes[0, 0], palette='viridis')
    axes[0, 0].set_title('Top-K Accuracy')
    axes[0, 0].set_ylim(0, 1)
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].set_xlabel('K')
    # Add value labels
    for i, v in enumerate(df_topk['Accuracy']):
        axes[0, 0].text(i, v + 0.02, f"{v:.1%}", ha='center')

    # Plot 2: MRR at different k values
    if 'mrr_at_k' in metrics and metrics['mrr_at_k']:
        df_mrr = pd.DataFrame({
            'K': list(metrics['mrr_at_k'].keys()),
            'MRR': list(metrics['mrr_at_k'].values())
        })
        df_mrr = df_mrr.sort_values('K')

        sns.barplot(x='K', y='MRR', data=df_mrr, ax=axes[0, 1], palette='viridis')
        axes[0, 1].set_title('Mean Reciprocal Rank (MRR) at Different K Values')
        axes[0, 1].set_ylim(0, max(1, df_mrr['MRR'].max() * 1.1))
        axes[0, 1].set_ylabel('MRR')
        axes[0, 1].set_xlabel('K')
        # Add value labels
        for i, v in enumerate(df_mrr['MRR']):
            axes[0, 1].text(i, v + 0.01, f"{v:.3f}", ha='center')
    else:
        axes[0, 1].axis('off')

    # Plot 3: Rank Distribution - New visualization
    if metrics['rank_distribution']:
        df_rank = pd.DataFrame([
            {'Rank': rank, 'Count': count}
            for rank, count in metrics['rank_distribution'].items()
        ])
        df_rank = df_rank.sort_values('Rank')

        sns.barplot(x='Rank', y='Count', data=df_rank, ax=axes[1, 0], palette='viridis')
        axes[1, 0].set_title('Distribution of Correct Answer Ranks')
        axes[1, 0].set_ylabel('Count')
        axes[1, 0].set_xlabel('Rank')

        # Add percentage labels
        total = df_rank['Count'].sum()
        for i, v in enumerate(df_rank['Count']):
            axes[1, 0].text(i, v + 0.5, f"{v / total:.1%}", ha='center')

    axes[1, 1].axis('off')

    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for the suptitle

    # Create a new figure for additional metrics
    fig2, axes2 = plt.subplots(2, 2, figsize=(15, 10))
    fig2.suptitle(f'Additional Evaluation Metrics{title_suffix}', fontsize=16)

    # Plot 5: Success Rate
    success_data = pd.DataFrame([
        {'Status': 'Success', 'Count': metrics['successful_queries']},
        {'Status': 'Failure', 'Count': metrics['total_queries'] - metrics['successful_queries']}
    ])

    ax5 = axes2[0, 0]
    wedges, texts, autotexts = ax5.pie(
        success_data['Count'],
        autopct='%1.1f%%',
        textprops={'fontsize': 12},
        colors=sns.color_palette('viridis', 2)
    )
    ax5.set_title('Success Rate')
    ax5.legend(wedges, success_data['Status'], loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

    # Plot 6: Leave empty since MRR@k chart covers this
    axes2[0, 1].axis('off')

    # Plot 7: Leave empty since overlap distribution was removed
    axes2[1, 0].axis('off')

    # Plot 8: Query Success by Index (to identify patterns in the dataset)
    df_results = pd.DataFrame([
        {'Index': r['index'], 'Success': 1 if r.get('success', False) else 0}
        for r in results if not r.get('impossible', False)  # Exclude impossible questions
    ])

    # Calculate rolling success rate
    window_size = min(50, len(df_results) // 5) if len(df_results) > 20 else 5
    if len(df_results) >= window_size:
        df_results['Rolling Success'] = df_results['Success'].rolling(window=window_size).mean()

        ax8 = axes2[1, 1]
        ax8.plot(df_results['Index'], df_results['Rolling Success'], 'b-')
        ax8.set_title(f'Rolling Success Rate (Window Size: {window_size})')
        ax8.set_xlabel('Query Index')
        ax8.set_ylabel('Success Rate')
        ax8.set_ylim(0, 1.05)
        ax8.grid(True)

    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust for the suptitle

    # Add a visualization for impossible questions if present
    if 'impossible_questions' in metrics and metrics['impossible_questions'] > 0:
        # Create a new figure
        fig_imp = plt.figure(figsize=(10, 6))
        fig_imp.suptitle(f'Impossible Question Verification{title_suffix}', fontsize=16)

        # Create data for pie chart
        imp_data = pd.DataFrame([
            {'Status': 'Correctly Rejected',
             'Count': metrics['impossible_questions'] - metrics.get('false_positives', 0)},
            {'Status': 'False Positives',
             'Count': metrics.get('false_positives', 0)}
        ])

        # Plot pie chart
        ax_imp = fig_imp.add_subplot(111)
        wedges, texts, autotexts = ax_imp.pie(
            imp_data['Count'],
            labels=imp_data['Status'],
            autopct='%1.1f%%',
            textprops={'fontsize': 12},
            colors=sns.color_palette('viridis', 2)
        )
        ax_imp.set_title('Impossible Question Verification')

        # Add count text
        for i, row in enumerate(imp_data.itertuples()):
            autotexts[i].set_text(f"{row.Count} ({autotexts[i].get_text()})")

    # Add rerank score visualization if reranking was used
    if metrics.get("reranking_enabled", False):
        # Create a new figure for reranking metrics
        fig_rerank = plt.figure(figsize=(15, 5))
        fig_rerank.suptitle('Cohere Reranking Analysis', fontsize=16)

        # Collect rerank scores
        rerank_scores_success = []
        rerank_scores_failure = []

        for result in results:
            if not result.get('impossible', False):  # Exclude impossible questions
                for chunk in result.get('retrieved_chunks', []):
                    if 'rerank_score' in chunk:
                        if result.get('success', False) and chunk.get('is_match', False):
                            rerank_scores_success.append(chunk['rerank_score'])
                        elif not result.get('success', False) and chunk['rank'] == 1:  # Best non-matching result
                            rerank_scores_failure.append(chunk['rerank_score'])

        # Plot 1: Rerank Score Distribution
        ax_rerank1 = fig_rerank.add_subplot(131)
        if rerank_scores_success and rerank_scores_failure:
            ax_rerank1.hist([rerank_scores_success, rerank_scores_failure],
                            bins=15, alpha=0.7, label=['Successful Matches', 'Failed Top Results'])
            ax_rerank1.set_xlabel('Rerank Score')
            ax_rerank1.set_ylabel('Count')
            ax_rerank1.set_title('Rerank Score Distribution')
            ax_rerank1.legend()

        # Plot 2: Rerank Score vs Success Rate
        ax_rerank2 = fig_rerank.add_subplot(132)
        if rerank_scores_success:
            avg_success_score = sum(rerank_scores_success) / len(rerank_scores_success)
            avg_failure_score = sum(rerank_scores_failure) / len(rerank_scores_failure) if rerank_scores_failure else 0

            ax_rerank2.bar(['Successful Matches', 'Failed Top Results'],
                           [avg_success_score, avg_failure_score],
                           color=sns.color_palette('viridis', 2))
            ax_rerank2.set_ylabel('Average Rerank Score')
            ax_rerank2.set_title('Average Rerank Scores by Outcome')

            # Add value labels
            ax_rerank2.text(0, avg_success_score + 0.01, f"{avg_success_score:.3f}", ha='center')
            if avg_failure_score > 0:
                ax_rerank2.text(1, avg_failure_score + 0.01, f"{avg_failure_score:.3f}", ha='center')

        # Plot 3: Rerank Score Threshold Analysis
        ax_rerank3 = fig_rerank.add_subplot(133)
        if rerank_scores_success and rerank_scores_failure:
            # Calculate precision/recall at different thresholds
            all_scores = sorted(rerank_scores_success + rerank_scores_failure, reverse=True)
            thresholds = []
            precisions = []

            for threshold in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:
                tp = sum(1 for s in rerank_scores_success if s >= threshold)
                fp = sum(1 for s in rerank_scores_failure if s >= threshold)
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                thresholds.append(threshold)
                precisions.append(precision)

            ax_rerank3.plot(thresholds, precisions, 'o-')
            ax_rerank3.set_xlabel('Rerank Score Threshold')
            ax_rerank3.set_ylabel('Precision')
            ax_rerank3.set_title('Precision vs Rerank Score Threshold')
            ax_rerank3.grid(True)

        plt.tight_layout(rect=[0, 0, 1, 0.96])

    # Save the figures
    if output_file:
        viz_file = os.path.splitext(output_file)[0] + "_viz.png"
        fig.savefig(viz_file)

        viz_file2 = os.path.splitext(output_file)[0] + "_viz_additional.png"
        fig2.savefig(viz_file2)

        if 'impossible_questions' in metrics and metrics['impossible_questions'] > 0:
            imp_viz_file = os.path.splitext(output_file)[0] + "_impossible_viz.png"
            fig_imp.savefig(imp_viz_file)
            print(f"Impossible question visualization saved to {imp_viz_file}")

        if metrics.get("reranking_enabled", False):
            rerank_viz_file = os.path.splitext(output_file)[0] + "_rerank_viz.png"
            fig_rerank.savefig(rerank_viz_file)
            print(f"Reranking analysis visualization saved to {rerank_viz_file}")

        print(f"Visualizations saved to {viz_file} and {viz_file2}")
    else:
        plt.show()


def is_substantial_overlap(text1: str, text2: str, threshold: float = 0.5) -> bool:
    """
    Check if there is substantial text overlap between two strings.

    Args:
        text1: First text
        text2: Second text
        threshold: Minimum overlap ratio to consider substantial

    Returns:
        True if overlap is above threshold
    """
    # Calculate word overlap
    return calculate_overlap(text1, text2) >= threshold


def calculate_overlap(text1: str, text2: str) -> float:
    """
    Calculate the word overlap ratio between two texts.

    Args:
        text1: First text
        text2: Second text

    Returns:
        Overlap ratio (0.0 to 1.0)
    """
    # Split into words and remove empty strings
    words1 = set(w.lower() for w in text1.split() if w)
    words2 = set(w.lower() for w in text2.split() if w)

    if not words1 or not words2:
        return 0.0

    # Find common words
    common_words = words1.intersection(words2)

    # Calculate overlap ratio based on the smaller set
    smaller_set = min(len(words1), len(words2))
    return len(common_words) / smaller_set if smaller_set > 0 else 0.0


"""
Debug script to add to evaluate_embeddings.py to diagnose source matching issues
"""


# Add this function to print the source files for all your questions
def debug_source_files(questions):
    """Print all source files found in the questions JSON to help with debugging."""
    print("\n===== DEBUGGING SOURCE FILES =====")
    all_sources = set()

    for i, question in enumerate(questions):
        source_files = question.get("sources", [])
        if not source_files:
            # For backward compatibility, check the old "source" field
            old_source = question.get("source", "")
            if old_source:
                source_files = [old_source]

        # Print the question, source files, and passage text
        print(f"Question {i + 1}: {question.get('question', '')[:70]}...")
        print(f"  Sources: {source_files}")
        for src in source_files:
            all_sources.add(src)

        # Print a sample of the passage text
        passage = question.get("passage", "")
        print(f"  Passage: {passage[:50]}...")
        print()

    debug_print("\nAll unique source files in questions:")
    for src in sorted(all_sources):
        debug_print(f"  - {src}")
    debug_print("===================================\n")


def main():
    parser = argparse.ArgumentParser(
        description="Evaluate vector search quality using questions generated by text_question_generator.py")
    
    # Core arguments
    parser.add_argument("--questions", required=True, help="Path to test questions JSON file (from text_question_generator.py)")
    parser.add_argument("--store", choices=["faiss", "pinecone", "chroma"], default="faiss", help="Vector store type")
    parser.add_argument("--top-k", type=int, default=4, help="Number of top results to consider")
    parser.add_argument("--output", help="Path to save report file")
    parser.add_argument("--brief", action="store_true", help="Generate brief report without details")
    parser.add_argument("--debug", action="store_true", help="Enable debug output")
    
    # Vector store specific arguments
    parser.add_argument("--faiss-path", help="FAISS index directory path")
    parser.add_argument("--pinecone-key", help="Pinecone API key")
    parser.add_argument("--pinecone-index", help="Pinecone index name")
    parser.add_argument("--chroma-path", help="Chroma database path")
    parser.add_argument("--chroma-index", help="Chroma collection name")
    
    # Reranking arguments
    parser.add_argument("--use-reranking", action="store_true", help="Enable reranking")
    parser.add_argument("--reranker-type", choices=["huggingface"], default="huggingface",
                        help="Type of reranker to use (huggingface only)")
    parser.add_argument("--rerank-model",
                        help="Rerank model name (default: BAAI/bge-reranker-large)")
    parser.add_argument("--rerank-top-n", type=int, default=100,
                        help="Number of documents to fetch for reranking")
    parser.add_argument("--device", help="Device for HuggingFace models (cpu, cuda, mps, etc.)")

    args = parser.parse_args()

    # Validate reranker requirements
    if args.use_reranking:
        if args.reranker_type == "huggingface" and not HUGGINGFACE_AVAILABLE:
            print("Error: HuggingFace reranking requires sentence-transformers.")
            print("Install with: pip install sentence-transformers torch")
            sys.exit(1)

    # Load questions
    questions = load_questions(args.questions)
    print(f"Loaded {len(questions)} questions for evaluation")

    # Load vector store
    print(f"Loading {args.store} vector store...")
    vector_store_loader = VectorStoreLoader()
    
    try:
        store_kwargs = {}
        if args.store == 'faiss':
            store_kwargs['faiss_path'] = args.faiss_path
        elif args.store == 'pinecone':
            store_kwargs['pinecone_key'] = args.pinecone_key
            store_kwargs['pinecone_index'] = args.pinecone_index
        elif args.store == 'chroma':
            store_kwargs['chroma_path'] = args.chroma_path
            store_kwargs['chroma_index'] = args.chroma_index
        
        vector_store_loader.load_vector_store(args.store, **store_kwargs)
        print(f"Successfully loaded {args.store} vector store")
        
    except Exception as e:
        print(f"Error loading vector store: {e}")
        sys.exit(1)
    
    # Run evaluation
    print("\nStarting retrieval evaluation...")
    results, metrics = evaluate_retrieval_quality(
        vector_store_loader,
        questions,
        args.top_k,
        args.output,
        not args.brief,
        args.debug,
        args.use_reranking,
        args.reranker_type,
        args.rerank_model,
        args.rerank_top_n,
        args.device
    )

    # Export evaluation data
    if args.output:
        # Export metrics overview
        metrics_file = os.path.splitext(args.output)[0] + "_metrics.json"
        with open(metrics_file, "w", encoding="utf-8") as f:
            json.dump(metrics, f, indent=2)
        print(f"Metrics saved to {metrics_file}")

        # Export detailed results as CSV
        results_file = os.path.splitext(args.output)[0] + "_results.csv"

        # Create a flattened version of results for CSV export
        flattened_results = []
        for result in results:
            # Basic result info
            result_row = {
                "index": result.get("index", ""),
                "question": result.get("question", ""),
                "query_type": result.get("query_type", ""),
                "expected_chunk_id": result.get("expected_chunk_id", ""),
                "found": result.get("found", False),
                "rank": result.get("rank", None),
                "success": result.get("success", False),
                "source_files": ";".join(result.get("source_files", [])),
                "reranked": result.get("reranked", False)
            }

            # Add rerank score of the best match if available
            if result.get("retrieved_chunks"):
                best_chunk = result["retrieved_chunks"][0]
                if "rerank_score" in best_chunk:
                    result_row["best_rerank_score"] = best_chunk["rerank_score"]
                if "similarity_score" in best_chunk:
                    result_row["best_similarity_score"] = best_chunk["similarity_score"]

            # Add to flattened results
            flattened_results.append(result_row)

        # Convert to DataFrame and save
        df_results = pd.DataFrame(flattened_results)
        df_results.to_csv(results_file, index=False)
        print(f"Detailed results saved to {results_file}")

        # Export chunk data for successful and failed retrievals
        chunks_file = os.path.splitext(args.output)[0] + "_chunks.csv"
        chunk_rows = []

        for result in results:
            result_index = result.get("index", "")
            success = result.get("success", False)
            expected_chunk_id = result.get("expected_chunk_id", "")

            for chunk in result.get("retrieved_chunks", []):
                chunk_row = {
                    "result_index": result_index,
                    "query_success": success,
                    "expected_chunk_id": expected_chunk_id,
                    "retrieved_chunk_id": chunk.get("chunk_id", ""),
                    "rank": chunk.get("rank", ""),
                    "source_file": chunk.get("source_file", ""),
                    "overlap": chunk.get("overlap", 0),
                    "is_match": chunk.get("is_match", False),
                    "similarity_score": chunk.get("similarity_score", 0)
                }

                # Add rerank score if available
                if "rerank_score" in chunk:
                    chunk_row["rerank_score"] = chunk["rerank_score"]

                chunk_rows.append(chunk_row)

        # Convert to DataFrame and save
        if chunk_rows:
            df_chunks = pd.DataFrame(chunk_rows)
            df_chunks.to_csv(chunks_file, index=False)
            print(f"Chunk details saved to {chunks_file}")

    # Print summary
    print(f"\n{'=' * 60}")
    print("EVALUATION SUMMARY")
    print(f"{'=' * 60}")
    print(f"Total questions: {metrics['total_queries']}")
    print(f"Successful retrievals: {metrics['successful_queries']}")
    print(f"Success rate: {metrics['success_rate']:.1%}")
    print(f"Mean Reciprocal Rank (MRR): {metrics['mrr']:.3f}")
    print(f"Hit@1: {metrics['hit_at_1']:.1%}")
    print(f"Hit@{args.top_k}: {metrics[f'hit_at_{args.top_k}']:.1%}")
    print(f"Average position of correct answers: {metrics['avg_position']:.2f}")
    print(f"Vector store: {args.store}")
    print(f"Embedding model: {vector_store_loader.embedding_model}")
    print(f"Reranking: {'Enabled' if metrics.get('reranking_enabled') else 'Disabled'}")
    if metrics.get('reranking_enabled'):
        print(f"Reranker type: {metrics.get('reranker_type')}")
        print(f"Rerank model: {metrics.get('rerank_model')}")
        
    # Show top-k accuracies
    print("\nTop-K Accuracies:")
    for k in range(1, min(6, args.top_k + 1)):
        key = f"top_{k}"
        if key in metrics['top_k_accuracy']:
            print(f"  Top-{k}: {metrics['top_k_accuracy'][key]:.1%}")


if __name__ == "__main__":
    main()
